{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "import math\n",
    "# importing required libraries\n",
    "import torch.nn.functional as F\n",
    "import math,copy,re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import random\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSentenceEncoder:\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    def encode_sentences(self, input_sentences):\n",
    "        tokenized_input = self.tokenizer(input_sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokenized_input)\n",
    "        encoded_sentences = outputs.last_hidden_state\n",
    "        return encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = [\"Your first sentence\", \"Your second sentence at the test\"]\n",
    "tmp = BERTSentenceEncoder()\n",
    "encoded_sentences = tmp.encode_sentences(input_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 8, 768])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerDecoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers, num_heads, hidden_size, max_length=200):\n",
    "        super(CustomTransformerDecoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_size, max_length)\n",
    "        decoder_layers = nn.TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_decoder(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pos_enc = torch.zeros((1, max_len, d_model))\n",
    "\n",
    "        pos_enc[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[0, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pos_enc', pos_enc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_enc[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TransformerDecoder.forward() missing 1 required positional argument: 'memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/speech/dbwork/mul/spielwiese4/students/desengus/codes/base_ipynb/english_any_lang_trans_102.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdestc0strapp15/speech/dbwork/mul/spielwiese4/students/desengus/codes/base_ipynb/english_any_lang_trans_102.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m input_tensor \u001b[39m=\u001b[39m encoded_sentences\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdestc0strapp15/speech/dbwork/mul/spielwiese4/students/desengus/codes/base_ipynb/english_any_lang_trans_102.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdestc0strapp15/speech/dbwork/mul/spielwiese4/students/desengus/codes/base_ipynb/english_any_lang_trans_102.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m output_tensor \u001b[39m=\u001b[39m custom_decoder(input_tensor)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdestc0strapp15/speech/dbwork/mul/spielwiese4/students/desengus/codes/base_ipynb/english_any_lang_trans_102.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInput shape:\u001b[39m\u001b[39m\"\u001b[39m, input_tensor\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdestc0strapp15/speech/dbwork/mul/spielwiese4/students/desengus/codes/base_ipynb/english_any_lang_trans_102.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOutput shape:\u001b[39m\u001b[39m\"\u001b[39m, output_tensor\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/speech/dbwork/mul/spielwiese4/students/desengus/miniconda3/envs/dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/speech/dbwork/mul/spielwiese4/students/desengus/miniconda3/envs/dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/speech/dbwork/mul/spielwiese4/students/desengus/codes/base_ipynb/english_any_lang_trans_102.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdestc0strapp15/speech/dbwork/mul/spielwiese4/students/desengus/codes/base_ipynb/english_any_lang_trans_102.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdestc0strapp15/speech/dbwork/mul/spielwiese4/students/desengus/codes/base_ipynb/english_any_lang_trans_102.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdestc0strapp15/speech/dbwork/mul/spielwiese4/students/desengus/codes/base_ipynb/english_any_lang_trans_102.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_decoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdestc0strapp15/speech/dbwork/mul/spielwiese4/students/desengus/codes/base_ipynb/english_any_lang_trans_102.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdestc0strapp15/speech/dbwork/mul/spielwiese4/students/desengus/codes/base_ipynb/english_any_lang_trans_102.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/speech/dbwork/mul/spielwiese4/students/desengus/miniconda3/envs/dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/speech/dbwork/mul/spielwiese4/students/desengus/miniconda3/envs/dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: TransformerDecoder.forward() missing 1 required positional argument: 'memory'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 768\n",
    "output_size = 109\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "hidden_size = 256  # Adjust this based on your specific needs\n",
    "max_length = 200\n",
    "\n",
    "# Instantiate the custom transformer decoder\n",
    "custom_decoder = CustomTransformerDecoder(input_size, output_size, num_layers, num_heads, hidden_size, max_length)\n",
    "\n",
    "# Example input tensor with shape [2, 8, 768]\n",
    "input_tensor = encoded_sentences\n",
    "\n",
    "# Forward pass\n",
    "output_tensor = custom_decoder(input_tensor)\n",
    "\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "print(\"Output shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
