{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/speech/dbwork/mul/spielwiese4/students/desengus/miniconda3/envs/dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define a simple model using PyTorch's TransformerDecoder\n",
    "class SimpleTransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(SimpleTransformerDecoderModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, batch_first=True), \n",
    "            num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        tgt = self.embedding(tgt) * np.sqrt(self.d_model)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer_decoder(tgt, memory)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:x.size(0), :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSentenceEncoder:\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    def encode_sentences(self, input_sentences):\n",
    "        tokenized_input = self.tokenizer(input_sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokenized_input)\n",
    "        encoded_sentences = outputs.last_hidden_state\n",
    "\n",
    "        # take only the CLS mode\n",
    "        context_vector = encoded_sentences[:, 0,:]\n",
    "        return tokenized_input, encoded_sentences, context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_mapping = {'0': 1, 'C0': 2, 'C0#': 3, 'D0': 4, 'D0#': 5, 'E0': 6, 'F0': 7, 'F0#': 8, 'G0': 9, 'G0#': 10, 'A0': 11, 'A0#': 12, 'B0': 13,\n",
    "                'C1': 14, 'C1#': 15, 'D1': 16, 'D1#': 17, 'E1': 18, 'F1': 19, 'F1#': 20, 'G1': 21, 'G1#': 22, 'A1': 23, 'A1#': 24, 'B1': 25,\n",
    "                'C2': 26, 'C2#': 27, 'D2': 28, 'D2#': 29, 'E2': 30, 'F2': 31, 'F2#': 32, 'G2': 33, 'G2#': 34, 'A2': 35, 'A2#': 36, 'B2': 37,\n",
    "                'C3': 38, 'C3#': 39, 'D3': 40, 'D3#': 41, 'E3': 42, 'F3': 43, 'F3#': 44, 'G3': 45, 'G3#': 46, 'A3': 47, 'A3#': 48, 'B3': 49,\n",
    "                'C4': 50, 'C4#': 51, 'D4': 52, 'D4#': 53, 'E4': 54, 'F4': 55, 'F4#': 56, 'G4': 57, 'G4#': 58, 'A4': 59, 'A4#': 60, 'B4': 61,\n",
    "                'C5': 62, 'C5#': 63, 'D5': 64, 'D5#': 65, 'E5': 66, 'F5': 67, 'F5#': 68, 'G5': 69, 'G5#': 70, 'A5': 71, 'A5#': 72, 'B5': 73,\n",
    "                'C6': 74, 'C6#': 75, 'D6': 76, 'D6#': 77, 'E6': 78, 'F6': 79, 'F6#': 80, 'G6': 81, 'G6#': 82, 'A6': 83, 'A6#': 84, 'B6': 85,\n",
    "                'C7': 86, 'C7#': 87, 'D7': 88, 'D7#': 89, 'E7': 90, 'F7': 91, 'F7#': 92, 'G7': 93, 'G7#': 94, 'A7': 95, 'A7#': 96, 'B7': 97,\n",
    "                'C8': 98, 'C8#': 99, 'D8': 100, 'D8#': 101, 'E8': 102, 'F8': 103, 'F8#': 104, 'G8': 105, 'G8#': 106, 'A8': 107, 'A8#': 108, 'B8': 109,'-1':100 }\n",
    "reverse_note_mapping = {v: k for k, v in note_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_file = pd.read_excel('/speech/dbwork/mul/spielwiese4/students/desengus/dry_crepe_pesto/excels/train/0.xlsx', index_col=0)\n",
    "training_words = tmp_file['words']\n",
    "training_words = [sentence.replace(';', ' ') for sentence in training_words]\n",
    "training_words[0] = '<BOS> ' + training_words[0]\n",
    "training_words[-1] = training_words[-1] + ' <EOS>'\n",
    "training_labels = [[note_mapping[note] for note in d.split(' ; ')] for d in tmp_file['mean_note_crepe']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item_no,i in enumerate(training_labels):\n",
    "    # print((training_labels[item_no]))\n",
    "    # print(type((training_labels[item_no])))\n",
    "    if len(i)<200:\n",
    "        if len(training_labels[item_no]) < 200:\n",
    "            training_labels[item_no].extend([100] * (200 - len(training_labels[item_no])))\n",
    "training_labels = torch.tensor(training_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item_no,i in enumerate(training_labels):\n",
    "#     # print((training_labels[item_no]))\n",
    "#     # print(type((training_labels[item_no])))\n",
    "#     if len(i)<200:\n",
    "#         if len(training_labels[item_no]) < 200:\n",
    "#             training_labels[item_no].extend([1] * (200 - len(training_labels[item_no])))\n",
    "# training_labels = torch.tensor(training_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/speech/dbwork/mul/spielwiese4/students/desengus/miniconda3/envs/dev/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "vocab_size = len(note_mapping)  # As there are 108 notes, 1 silence\n",
    "d_model = 768  # smaller d_model for simplicity\n",
    "nhead = 8  # ensure d_model is divisible by nhead\n",
    "num_layers = 3\n",
    "dim_feedforward = 768\n",
    "\n",
    "# Model\n",
    "encoder = BERTSentenceEncoder()\n",
    "decoder = SimpleTransformerDecoderModel(vocab_size, d_model, nhead, num_layers, dim_feedforward)\n",
    "\n",
    "# Set up optimizer and loss function\n",
    "optimizer = AdamW(decoder.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_input, encoded_sentences, context_vector = encoder.encode_sentences(training_words)\n",
    "_, encoded, _ = encoder.encode_sentences(training_words)\n",
    "# output = decoder(training_labels, encoded)\n",
    "#one_hot_encoded_output = torch.nn.functional.one_hot(training_labels, num_classes=len(note_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = decoder(training_labels, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([10, 56, 768])\n",
      "torch.Size([10, 200, 110])\n"
     ]
    }
   ],
   "source": [
    "print(len(training_words))\n",
    "#print(tokenized_input)\n",
    "print(encoded.shape)\n",
    "print(output.shape)\n",
    "#print(one_hot_encoded_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical CrossEntropy Loss: 4.696470260620117\n"
     ]
    }
   ],
   "source": [
    "# Reshape predictions to match the shape of targets\n",
    "predictions = output.view(-1, vocab_size).detach()\n",
    "targets = training_labels.view(-1)\n",
    "\n",
    "# Define the CrossEntropyLoss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(predictions, targets)\n",
    "\n",
    "print(\"Categorical CrossEntropy Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
